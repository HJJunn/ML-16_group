7.앙상블 학습과 포레스트

1. 투표기반 분류기
더 좋은 분류기를 만들기 위해서 각 분류기의 예측을 모아서 가장 많이 선택된 클래스로 예측하는 것을 직접투표 분류기라고 한다.
직접 투표 분류기는 많은 약한 학습기들이 모여 강한 학습기가 되어 개별 분류기보다 더 정확한 예측을 할 수 있다.
다음 코드에서 랜덤포레스트, 로지스틱 회귀, SVC 분류기들을 조합하여 투표 기반 분류기를 훈련하였다.
log_clf  = LogisticRegression()
rnd_clf = RandomForestClassifier()
svm_clf = SVC()

log_clf =  LogisticRegression(solver = 'lbfgs', random_state = 42)
rnd_clf = RandomForestClassifier(n_estimators =100, random_state = 42)
svm_clf = SVC(gamma = 'scale', random_state=42)

voting_clf = VotingClassifier(
    estimators = [('lr', log_clf), ('rf', rnd_clf),('svc', svm_clf)],
    voting = 'hard')
voting_clf.fit(X_train, y_train)
그 결과 투ㅍ 기반 분류기가 개별 분류기보다 성능이 높은 걸 확인할 수 있다.
LogisticRegression 0.864
RandomForestClassifier 0.896
SVC 0.896
VotingClassifier 0.912

간접투표는 개별 분류기의 예측을 평균 내어 확률이 가장 높은 클래스를 예측할 수 있다. 확률이 높은 투표에 비중을 두어서 직접 투표 방식보다 성능이 더욱 좋다.
간접 투표 코드는 voting = 'hard' 를 voting = 'soft'로 바꾸고 SVC는 probability 매개변수를 True로 지정해야 한다.
log_clf = LogisticRegression(solver='lbfgs', random_state = 42)
rnd_clf = RandomForestClassifier(n_estimators = 100 , random_state = 42)
svm_clf = SVC(gamma = 'scale',probability=True, random_state = 42)

voting_clf = VotingClassifier(
    estimators = [('rf', rnd_clf),('lr', log_clf),('svc', svm_clf)],
    voting = 'soft'
)
voting_clf.fit(X_train, y_train)

2. 배깅과 페이스팅
배깅(bootstrap aggregation) – 알고리즘은 같으나 훈련 세트의 서브셋을 무작위로 구성하여 분류기를 각각 다르게 하여 세팅하는 기법
이 과정에서 중복을 허용 여부에 따라 배깅(true) / 페이스팅(false)으로 분류한다. 이 때, 페이스팅은 훈련 샘플을 여러 개의 예측기에 걸쳐 사용할 수 있지만
배깅은 같은 훈련 샘플을 여러번 샘플링 할 수 있다.
분류 모델일 경우에 직접 투표를 통해 통계적 최빈값을 이용하여 예측하고 회귀 모델일 경우 평균을 계산하여 예측한다.
앙상블 학습의 경우 편향은 비슷하지만 분산은 줄어든다!!

배깅과 페이스팅은 분류일 경우 BaggingClassifier 함수를 사용한다.(페이스팅일 경우 bootstrap = False로 지정), n_jobs 매개변수는 CPU 코어 수를 지정.
 bag_clf = BaggingClassifier(
    DecisionTreeClassifier(), n_estimators = 500,
    max_samples = 100, bootstrap = True, n_jobs = -1)
bag_clf.fit(X_train, y_train)
y_pred = bag_clf.predict(X_test)

3. oob평가
Out-of-bag 평가: 배깅 시에 여러 번 샘플링된 데이터가 있고 한번도 사용되지 않은 데이터가 존재하는데 이 중 사용되지 않은 데이터를 이용해 모델을 평가하는 방법이다.
위 코드에서 oob_score = True로 지정하면 훈련이 끝난 후 oob 평가를 수행한다.
oob_decision_function_ 변수로 oob 샘플에 대한 결정함수의 값을 확인할 수 있다.

4. 랜덤 포레스트
랜덤 포레스트는 일반적으로 배깅 방법을 적용한 결정트리 앙상블.
rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes = 16, random_state = 42)
rnd_clf.fit(X_train, y_train)
y_pred_rf = rnd_clf.predict(X_test)
랜덤 포레스트에선 DecisionClassifier 대신 RandomForestClassifier 사용(max_samples로 훈련세트 크기 지정)
랜덤 포레스트 알고리즘은 무작위로 선택한 특성 후보 중에서 최적의 특성을 찾는 식으로 무작위성 주입 => 트리 더욱 다양해짐, 분산을 낮추어 전체적으로 훌륭한 모델 생성.

5.부스팅
부스팅(boosting): 앙상블 기법 중 하나로 성능이 약한 학습기 여러 개를 연결하여 강한 성능의 학습기로 결합하는 방법 [순차적이므로, 확장성이 떨어짐] (에이다부스트 / 그레디언트 부스팅)

에이다 부스트(Adaptive + Boosting) - 약한 분류기들이 상호 보완하도록 순차적으로 학습하고, 이들을 조합하여 최종적으로 강한 분류기의 성능을 향상시키는 것이다.
학습된 분류기에서 제대로 분류한 결과와 잘못 학습된 데이터를 다음 분류기에 전달하면 그 분류기에서 잘못 학습된 데이터의 가중치르 높인다. 이어서 업데이트된 가중치를 사용해 훈련세트에서
훈련하고 다시 예측을 만든다. 이렇게 예측 성능이 낮은 약한 분류기들을 조합하여서 최종적으로 조금 더 성능이 좋은 강한 분류기를 하나 만든다.

경사 하강법과의 차이점: 경사하강법은 비용함수를 최소화하기 위해 한 예측기의 모델 파라미터를 조정해가는 반면 에이다 부스트는 점차 더 좋아지도록 앙상블에 예측기를 추가한다.
실습코드:
from sklearn.ensemble import AdaBoostClassifier

ada_clf = AdaBoostClassifier(
    DecisionTreeClassifier(max_depth=1), n_estimators=200,
    algorithm="SAMME.R", learning_rate=0.5, random_state=42)
ada_clf.fit(X_train, y_train)

그레디언트 부스팅
에이다 부스트 처럼 예측기를 순차적으로 추가하지만 반복마다 샘플의 가중치를 수정하는 대신 이전 예측기가 만든 잔여 오차에 새로운 예측기를 학습시킨다.
그레디언트 트리 부스팅(GBRT)을 통해 회귀 모델 학습 이 때 GradientBoostingRegressor를 사용하여 훈련(n_estimators, max_depth, min_samples_leaf, learning_late등의 매개변수 사용)
learning_rate는 각 트리의 기여 정도를 조정한다. 0.1인 경우 많은 트리가 필요하지만, 예측의 성능은 좋아진다.(축소 방법)
코드:
from sklearn.ensemble import GradientBoostingRegressor

gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)
gbrt.fit(X, y)
