비지도 학습
현실 세계의 가공을 거치지 않은 데이터들은 대개 라벨링이 되어있지 않은데 라벨링 해야만 학습할 수 있는 
지도 학습과 다르게 데이터 속 패턴이나 규칙을 찾아 학습하는 비지도 학습이 대두됨.
비지도 학습을 진행할 경우, 지도 학습을 위해 소모되는 라벨링 처리과정이 크게 단축됨.
(실제로 데이터 전처리 과정에서 가장 큰 시간을 소요하는 작업이 데이터 라벨링 작업)
비지도 학습의 알고리즘으로 크게 차원축소(주성분분석), 이상치 탐지, 군집이 존재함.

군집은 고객분류, 데이터 분석, 차원축소, 이상치 탐지, 준지도 학습, 검색 엔진, 이미지 분할 등 다양한 애플리케이션에서 활용된다.

k-평균 알고리즘
이 알고리즘은 각 클러스터의 센트로이드라 부르는 특정 포인트를 찾고 가장 가까운 클러스터에 샘플을 할당한다.
군집에서 분류 알고리즘과는 다르게 각 샘플의 레이블은 알고리즘이 샘플에 할당한 클러스터의 인덱스이다.
즉 KMeans 클래스의 인스터는 labels_ 인스턴스 변수에 훈련된 샘플에 레이블을 가지고 있다.
y_pred is kmeans.labels_를 하면 True라는 결과가 나온다.
소프트 군집은 클러스터마다 샘플에 점수를 부여한다. 이 점수는 샘플과 센트로이드 사이의 거리가 될 수 있다.
transform() 메소드는 샘플과 각 센트로이드 사이의 거리를 반환합니다. 
고차원 데이터셋을 이런 방식으로 변환하는 것은 매우 효율적인 비선형 차원 축소 기법이 될 수 있다.

작동방식
레이블이나 센트로이드가 주어지지 않는 경우, 샌트로이드를 랜덤하게 선정한다.
그 다음 샘플에 레이블을 할당하고 센트로이드를 업데이트 하는 것을 센트로이드에 변화가 없을 때까지 반복한다.
이 알고리즘은 수렴하는 것이 보장되지만 적절한 솔루션으로 수렴하지 못할 수 있다.
이 여부는 센트로이드 초기화에 달려있다.

센트로이드 초기화 방법
1. init 매개변수에 센트로이드 리스트를 담은 넘파이 배열을 저장하고 n_init를 1로 설정한다.
2. 랜덤 초기화를 다르게 하여 여러 번 알고리즘을 실행한다.
랜덤 초기화 횟수는 n_init 매개변수로 조절한다.(기본값 10)
최선의 솔루션을 알 수 있게 이너셔라는 지표를 사용하는데 이너션란 각 샘플과 가장 가까운 센트로이드 사이의 평균 제곱 거리이다.
KMeans 클래스는 알고리즘을 n_init번 실행하여 이너셔가 가장 낮은 모델을 반환한다.

미니배치 K- 평균
미니배치 k-평균 알고리즘은 일반 k-평균 알고리즘보다 훨씬 빠르지만 클러스터의 개수가 증가할 때 이너셔는 일반적으로 더 나쁘다.
k가 증가함에 따라 이너셔가 점점 줄어들어 두 곡선의 차이가 차지하는 비율이 점점 커진다.

최적의 클러스터 개수 찾기
이너셔를 k의 함수로 그래프를 그려보면 k가 증가함에 따라 이너셔가 빠르게 줄어드는 구간을 엘보라고 한다.
더 정확히 k값을 찾는 방법으로 실루엣 점수가 있다.
이 값은 모든 샘플에 대한 실루엣 계수의 평균으로 (b-a)/max(a,b)로 계산한다.
a는 동일한 클러스터에 있는 다른 샘플까지의 평균 거리이고, b는 가장 가까운 클러스터까지 평균거리이다.
실루엣 계수는 -1 부터 +1까지 바뀌는데 +1에 가까우면 자신의 클러스터 안에 잘 속해 있는 것이고, 0일 경우 클러스터 경계에 위치한다는 의미이다
-1에 가까운 경우는 이 샘플이 잘못된 클러스터에 할당되었다는 의미이다.
실루엣 다이어그램을 통해 샘플의 개수, 실루엣 계수, 실루엣 점수를 나타낼 수 있다.(높이, 너비, 수직파선)
샘플이 파선의 왼쪽에서 멈추면 클러스터의 샘플이 다른 클러스터랑 너무 가깝다는 것을 의미한다.
전반적으로 실루엣 점수가 더 높더라도 비슷한 크기의 클러스터를 얻을 수 있는 k값을 선택하는 것이 좋다.

k-평균은 속도가 빠르고 확장이 용이하다는 장점이 있지만 알고리즘을 여러번 수행해야 하고, 클러스터 개수를 지정해주어야 한다.
또한, 클러스터의 크기나 밀집도가 서로 다르거나 원형이 아닐경우 잘 작동하지 않으므로 다른 군집 알고리즘을 사용할 수 있다.
예를 들어 타원형 클러스터에서는 가우시안 혼합 모델이 잘 작동한다.

군집의 활용
1)색상 분할 
image = imread(os.path.join("images", "unsupervised_learning", "ladybug.png"))
image.shape
(533, 800, 3) -> 첫번째 차원: 높이, 두번째 : 너비, 세번째: 컬러 채널의 개수

2)전처리
1.데이터 셋을 불러와 훈련세트와 데이터 세트로 분할 후 로지스틱 회귀모델로 훈련
2. 파이프라인을 만들어 훈련 세트를 50개의 클러스터로 모은 뒤 이미지를 클러스터까지 거리로 바꾼다.
3. GridSearchCV를 사용해 최적의 클러스터 개수를 찾는다.

3)준지도 학습
1.각 클러스터에서 센트로이드에 가장 가까운 이미지를 찾는다. (대표 이미지)
2.레이블을 동일한 클러스터에 있는 모든 샘플로 전파한다.(레이블 전파)
3.잘못 부여된 레이블이 있을 수 있으므로 센트로에드와 가까운 샘플의 20%에만 레이블을 전파한다.

DBSCAN
이 알고리즘은 군집 알고리즘 중 하나로 데이터 간 거리기반 알고리즘인 K-평균과는 다르게 밀도를 기반으로 군집화 작업을 수행하는 알고리즘이다.
밀도 기반으로 진행하다 보니 데이터간 거리가 멀더라도 같은 군집으로 포함하는 경우가 있으며 이에 따라 분포가 깔끔한 형태를 띄는 경우가 많다.
DBSCAN 알고리즘은 클러스터의 모양과 개수에 상관없이 감지할 수 있는 능력이 있으며 이상치에 안정적이고 하이퍼파라미터가 두개이다.(eps, min_samples)
하지만, 클러스터 간의 밀집도가 크게 다르면 모든 클러스터를 올바르게 잡아내는 것이 불가능하다.
DBSCAN 클래스는 predict() 메소드를 제공하지 않고 fit_predict() 메소드를 제공한다.
KNeighborsClassifier의 kneighbors() 메소드를 사용하여 가장 가까운 k개 이웃의 거리와 인뎃를 반환한다.
y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)
y_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx]
y_pred[y_dist > 0.2] = -1
y_pred.ravel()

가우시안 혼합 모델
가우시안(=정규분포) 모델을 여러 개 혼합해 군집화하는 모델로 평균값, 공분산 같은 파라미터를 이용해 추정하기 때문에 K-평균과 다르게 데이터의 분산을 고려할 수 있다는 장점이 존재함.
베이즈 가우시안 혼합 모델
가우시안 혼합 모델에서 베이즈 정리를 이용해 최적 군집 수를 찾아주는 추가적인 모델도 존재함.
베이즈 정리는 X가 주어졌을 때 z의 조건부 확률인 사후확률 분로를 계산한다. p(z|X)
가우시안 혼합 모델에서 분모인 p(X)는 가능한 모든 z값에 대해 적분해야 하므로 계산하기 어렵다
이를 해결하는 방법 중 하나가 변분 파라미터를 가진 분포 패밀리를 선택한 후 q(z)가 좋은 근삿값이 되도록 파라미터를 최적화한다.

느낀점 
데이터 분석은 지도 학습과 비지도 학습으로 나뉘는데 지도 학습은 분석 알고리즘이 매우 다양하지만 비지도 학습은 대표적인 k-means 알고리즘만 알고 있었다.
그런데 이번에 비지도 학습을 배우면서 군집이 많은 데에서 활용이 되고 DBSCAN, 가우시안 혼합 등의 모델이 있다는 것을 알게 되었다.
k-means 알고리즘을 사용하였을 때 센트로이드나 이너셔 등의 용어나 지표등에서 잘 알지 못하였다.
그리고 k값을 구할때 엘보 값을 이용해 구하였는데 이 방법은 잘못된 방법이였다는 것을 알게 되었고
최선의 클러스터 개수를 찾기 위해서 실루엣 점수를 사용하고 실루엣 다이어그램을 활용해 이를 확인하고 k값을 선정하는 방법에 대해 알 수 있었다.
생성 ai가 매우 발달하고 있는 추세에서 군집을 활용한 이미지 분할과 MNIST같은 데이터셋을 전처리 하는데 이를 활용해 볼 수 있다.
가우시안 혼합 모델(GMM)과 베이즈 가우시안 혼합 모델 같은 경우에는 내용이 많고 어려워 추후에 좀 더 깊게 공부해야 될 것 같다.


